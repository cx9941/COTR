{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import aggregate_expert_opinions\n",
    "metric_results = {}\n",
    "llm_type = 'gpt'\n",
    "num_seed = 1\n",
    "for dataset_name in ['en', 'eu', 'jp']:\n",
    "    gold = pd.read_csv(f'../step5-close_llm_baseline/data/{dataset_name}.csv')[['job_title', 'job_description', 'task', 'bert_task']]\n",
    "    gold['bert_task'] = gold['bert_task'].apply(eval)\n",
    "    for seed in range(num_seed):\n",
    "        ans = []\n",
    "        for turn in range(8):\n",
    "            df = pd.read_csv(f'../step5-close_llm_baseline/results/{dataset_name}/{llm_type}/seed{seed}/result{turn}.csv') \n",
    "            ans.append(df)\n",
    "        ans = pd.concat(ans)\n",
    "        ans.columns = ['job_title', 'job_description', f'{llm_type}_task_seed{seed}']\n",
    "        ans[f'{llm_type}_task_seed{seed}'] = ans[f'{llm_type}_task_seed{seed}'].apply(eval)\n",
    "        gold = pd.merge(gold, ans, on=['job_title', 'job_description'])\n",
    "    gold['gpt_task'] = gold.apply(lambda x: aggregate_expert_opinions([x[f'{llm_type}_task_seed{i}'] for i in range(num_seed)], top_num=10)[0], axis=1)\n",
    "    gold = gold[['job_title', 'job_description', 'task', f'{llm_type}_task', 'bert_task']]\n",
    "    gold.to_csv(f'results/{dataset_name}/{dataset_name}-{llm_type}.csv', index=None)\n",
    "    gold.to_excel(f'results/{dataset_name}/{dataset_name}-{llm_type}.xlsx', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in ['eu']:\n",
    "    gold = pd.read_excel(f'results/{dataset_name}/{dataset_name}-{llm_type}-annotated.xlsx')\n",
    "    gold['bert_task'] = gold['bert_task'].apply(eval)\n",
    "    gold['gpt_task'] = gold['gpt_task'].apply(eval)\n",
    "    gold = gold.fillna(0)\n",
    "    gold['task'] = gold.apply(lambda x: x['gpt_task'][0] if x['annotated'] == 1 else x['task'], axis=1)\n",
    "    gold = gold.drop('annotated', axis=1)\n",
    "    gold.to_csv(f'results/{dataset_name}/{dataset_name}-{llm_type}.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>eu</th>\n",
       "      <th>jp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hits@5</th>\n",
       "      <td>0.654135</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.203883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hits@10</th>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.349515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               en    eu        jp\n",
       "hits@5   0.654135  0.44  0.203883\n",
       "hits@10  0.684211  0.52  0.349515"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "metric_results = {}\n",
    "llm_type = 'gpt'\n",
    "for dataset_name in ['en', 'eu', 'jp']:\n",
    "    metric_results[dataset_name] = {}\n",
    "    ans = pd.read_csv(f'results/{dataset_name}/{dataset_name}-{llm_type}.csv')\n",
    "    ans[f'{llm_type}_task'] = ans[f'{llm_type}_task'].apply(eval)\n",
    "    for k in [5, 10]:\n",
    "        hits = ans.apply(lambda x: x['task'] in x[f'{llm_type}_task'][:k], axis=1).sum() / len(ans)\n",
    "        metric_results[dataset_name][f\"hits@{k}\"] = hits\n",
    "pd.DataFrame(metric_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>eu</th>\n",
       "      <th>jp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hits@5</th>\n",
       "      <td>0.255639</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.147059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hits@10</th>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.205882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               en    eu        jp\n",
       "hits@5   0.255639  0.16  0.147059\n",
       "hits@10  0.315789  0.24  0.205882"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "metric_results = {}\n",
    "for dataset_name in ['en', 'eu', 'jp']:\n",
    "    metric_results[dataset_name] = {}\n",
    "    gold = pd.read_csv(f'results/{dataset_name}/{dataset_name}-{llm_type}.csv')\n",
    "    gold['bert_task'] = gold['bert_task'].apply(eval)\n",
    "    for k in [5, 10]:\n",
    "        hits_10 = gold.apply(lambda x: x['task'] in x[f'bert_task'][:k], axis=1).sum() / len(gold)\n",
    "        metric_results[dataset_name][f\"hits@{k}\"] = hits_10\n",
    "pd.DataFrame(metric_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
